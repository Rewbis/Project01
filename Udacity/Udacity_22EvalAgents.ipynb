{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6426b820-d836-4542-929f-62275a4d13dc",
   "metadata": {},
   "source": [
    "## Summary: Evaluating RAG and Agent Performance with RAGAS \n",
    "Overview\n",
    "This demo walks through building a simple Retrieval-Augmented Generation (RAG) pipeline, converting it into a LangGraph workflow with tool use, and evaluating both using RAGAS, a framework for measuring the quality and correctness of RAG systems and agent responses.\n",
    "\n",
    "Key Steps Covered  \n",
    "1. Building a Simple RAG Pipeline  \n",
    "a. Document Setup  \n",
    "A list of five documents is created, covering topics like Meta, Nvidia, Google, Intel, and Dell.\n",
    "\n",
    "Each Document object includes metadata such as company and topic.\n",
    "\n",
    "\n",
    "documents = [\n",
    "\n",
    "  Document(page_content=\"Meta drops multimodal Llama 3.2...\", metadata={\"company\": \"Meta\", \"topic\": \"llama\"}),\n",
    "\n",
    "  ...\n",
    "\n",
    "]\n",
    "b. Chroma Vector Store  \n",
    "A Chroma vector store is created using OpenAI embeddings and documents are added to it.\n",
    "\n",
    "vector_store = Chroma(collection_name=\"udacity\", embedding_function=OpenAIEmbeddings())\n",
    "\n",
    "vector_store.add_documents(documents, ids=...)\n",
    "c. RAG Chain  \n",
    "A simple prompt-based chain is constructed:\n",
    "\n",
    "Prompt: \"Answer the question based only on the following context: {context}\"\n",
    "\n",
    "Chain: Prompt → LLM → Output parser\n",
    "\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "d. Invocation Example  \n",
    "Query: \"Who is partnering with Nvidia?\"\n",
    "\n",
    "Answer returned: \"Dell\" — correctly retrieved from the document set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c06975a8-6cf0-4fe3-a29e-2376be02a175",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\PythonProjects\\Project01\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Failed to initialize: Bad git executable.\nThe git executable must be specified in one of the following ways:\n    - be included in your $PATH\n    - be set via $GIT_PYTHON_GIT_EXECUTABLE\n    - explicitly set via git.refresh(<full-path-to-git-executable>)\n\nAll git commands will error until this is rectified.\n\nThis initial message can be silenced or aggravated in the future by setting the\n$GIT_PYTHON_REFRESH environment variable. Use one of the following values:\n    - quiet|q|silence|s|silent|none|n|0: for no message or exception\n    - warn|w|warning|log|l|1: for a warning message (logging level CRITICAL, displayed by default)\n    - error|e|exception|raise|r|2: for a raised exception\n\nExample:\n    export GIT_PYTHON_REFRESH=quiet\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\PythonProjects\\Project01\\.venv\\Lib\\site-packages\\git\\__init__.py:296\u001b[39m\n\u001b[32m    295\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m296\u001b[39m     \u001b[43mrefresh\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m _exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\PythonProjects\\Project01\\.venv\\Lib\\site-packages\\git\\__init__.py:287\u001b[39m, in \u001b[36mrefresh\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    285\u001b[39m GIT_OK = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mGit\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrefresh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    288\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\PythonProjects\\Project01\\.venv\\Lib\\site-packages\\git\\cmd.py:860\u001b[39m, in \u001b[36mGit.refresh\u001b[39m\u001b[34m(cls, path)\u001b[39m\n\u001b[32m    859\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m860\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(err)\n\u001b[32m    861\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mImportError\u001b[39m: Bad git executable.\nThe git executable must be specified in one of the following ways:\n    - be included in your $PATH\n    - be set via $GIT_PYTHON_GIT_EXECUTABLE\n    - explicitly set via git.refresh(<full-path-to-git-executable>)\n\nAll git commands will error until this is rectified.\n\nThis initial message can be silenced or aggravated in the future by setting the\n$GIT_PYTHON_REFRESH environment variable. Use one of the following values:\n    - quiet|q|silence|s|silent|none|n|0: for no message or exception\n    - warn|w|warning|log|l|1: for a warning message (logging level CRITICAL, displayed by default)\n    - error|e|exception|raise|r|2: for a raised exception\n\nExample:\n    export GIT_PYTHON_REFRESH=quiet\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image, display\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mragas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EvaluationDataset\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mragas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m evaluate\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mragas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LangchainLLMWrapper\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\PythonProjects\\Project01\\.venv\\Lib\\site-packages\\ragas\\__init__.py:6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mragas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataset_schema\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EvaluationDataset, MultiTurnSample, SingleTurnSample\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mragas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevaluation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m aevaluate, evaluate\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mragas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperiment\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Experiment, experiment, version_experiment\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mragas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrun_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RunConfig\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\PythonProjects\\Project01\\.venv\\Lib\\site-packages\\ragas\\experiment.py:9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mt\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgit\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpydantic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseModel\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\PythonProjects\\Project01\\.venv\\Lib\\site-packages\\git\\__init__.py:298\u001b[39m\n\u001b[32m    296\u001b[39m     refresh()\n\u001b[32m    297\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m _exc:\n\u001b[32m--> \u001b[39m\u001b[32m298\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mFailed to initialize: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m\"\u001b[39m.format(_exc)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_exc\u001b[39;00m\n\u001b[32m    300\u001b[39m \u001b[38;5;66;03m# } END initialize git executable path\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: Failed to initialize: Bad git executable.\nThe git executable must be specified in one of the following ways:\n    - be included in your $PATH\n    - be set via $GIT_PYTHON_GIT_EXECUTABLE\n    - explicitly set via git.refresh(<full-path-to-git-executable>)\n\nAll git commands will error until this is rectified.\n\nThis initial message can be silenced or aggravated in the future by setting the\n$GIT_PYTHON_REFRESH environment variable. Use one of the following values:\n    - quiet|q|silence|s|silent|none|n|0: for no message or exception\n    - warn|w|warning|log|l|1: for a warning message (logging level CRITICAL, displayed by default)\n    - error|e|exception|raise|r|2: for a raised exception\n\nExample:\n    export GIT_PYTHON_REFRESH=quiet\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "from langgraph.graph.message import MessagesState\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from IPython.display import Image, display\n",
    "from dotenv import load_dotenv\n",
    "from ragas import EvaluationDataset\n",
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness\n",
    "from ragas import messages as ragas_messages\n",
    "from ragas.integrations.langgraph import convert_to_ragas_messages\n",
    "from ragas.dataset_schema import MultiTurnSample\n",
    "from ragas.metrics import ToolCallAccuracy\n",
    "from ragas.metrics import AgentGoalAccuracyWithReference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21051f07-754d-4533-ad87-9a112c8f9475",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf04d7e-ac6f-4f82-93a7-071c3dbf71f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple RAG\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"Meta drops multimodal Llama 3.2 — here's why it's such a big deal\",\n",
    "        metadata={\"company\":\"Meta\", \"topic\": \"llama\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Chip giant Nvidia acquires OctoAI, a Seattle startup that helps companies run AI models\",\n",
    "        metadata={\"company\":\"Nvidia\", \"topic\": \"acquisition\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Google is bringing Gemini to all older Pixel Buds\",\n",
    "        metadata={\"company\":\"Google\", \"topic\": \"gemini\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"The first Intel Battlmage GPU benchmarks have leaked\",\n",
    "        metadata={\"company\":\"Intel\", \"topic\": \"gpu\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Dell partners with Nvidia to accelerate AI adoption in telecoms\",\n",
    "        metadata={\"company\":\"Dell\", \"topic\": \"partnership\"}\n",
    "    ),\n",
    "]\n",
    "\n",
    "ids = [\"id1\", \"id2\", \"id3\", \"id4\", \"id5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fb7e9b-40d8-44b8-a55e-a224e1029a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma(\n",
    "    collection_name=\"udacity\",\n",
    "    embedding_function=OpenAIEmbeddings(),\n",
    ")\n",
    "\n",
    "vector_store.add_documents(documents=documents, ids=ids)\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556de1c3-648e-4a78-82a6-792ea5316d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057c3751-d3f1-453c-9d4b-f11bcf199374",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the question based only on the following context:\n",
    "    {context}\n",
    "\n",
    "    Question: {query}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccc4461-9607-4b0d-b7a9-0a688f98a89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(relevant_docs):\n",
    "    return \"\\n\".join(doc.page_content for doc in relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4578c741-4cb4-44de-a12d-857036d01f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who is partnering with Nvidia?\"\n",
    "relevant_docs = retriever.invoke(query)\n",
    "chain.invoke({\"context\": format_docs(relevant_docs), \"query\": query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb49f735-a8df-4b0e-8c2e-372220114bd1",
   "metadata": {},
   "source": [
    "## 2. Evaluating the RAG Pipeline with RAGAS  \n",
    "a. Evaluation Dataset Creation  \n",
    "Five example queries and expected responses are prepared.\n",
    "\n",
    "For each, relevant documents are retrieved and responses generated.\n",
    "\n",
    "A EvaluationDataset is created using these entries.\n",
    "\n",
    "\n",
    "evaluation_dataset = EvaluationDataset.from_list(dataset)\n",
    "b. Metrics Used  \n",
    "RAGAS metrics include:\n",
    "\n",
    "LLMContextRecall\n",
    "\n",
    "Faithfulness\n",
    "\n",
    "FactualCorrectness\n",
    "\n",
    "c. Execution  \n",
    "evaluate() is called with the evaluation dataset and the wrapped LLM.\n",
    "\n",
    "Result:\n",
    "\n",
    "100% context recall\n",
    "\n",
    "100% faithfulness\n",
    "\n",
    "~54% factual correctness\n",
    "\n",
    "\n",
    "result = evaluate(\n",
    "\n",
    "  dataset=evaluation_dataset,\n",
    "\n",
    "  metrics=[LLMContextRecall(), Faithfulness(), FactualCorrectness()],\n",
    "\n",
    "  llm=evaluator_llm,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c0d6a6-a507-46b0-82e7-914535541e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_queries = [\n",
    "    \"What is Meta's latest development in AI?\",\n",
    "    \"Which company did Nvidia acquire?\",\n",
    "    \"What AI feature is Google adding to older Pixel Buds?\",\n",
    "    \"What recent information has leaked about Intel GPUs?\",\n",
    "    \"Which company did Dell partner with to accelerate AI adoption?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0679fb-1ea2-4f92-890f-754f7b80d900",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_responses = [\n",
    "    \"Meta drops multimodal Llama 3.2 — here's why it's such a big deal\",\n",
    "    \"Chip giant Nvidia acquires OctoAI, a Seattle startup that helps companies run AI models\",\n",
    "    \"Google is bringing Gemini to all older Pixel Buds\",\n",
    "    \"The first Intel Battlemage GPU benchmarks have leaked\",\n",
    "    \"Dell partners with Nvidia to accelerate AI adoption in telecoms\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14c876e-88d5-4285-b187-8962be2f7174",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "for query, reference in zip(sample_queries, expected_responses):\n",
    "    relevant_docs = retriever.invoke(query)\n",
    "    response = chain.invoke(\n",
    "        {\n",
    "            \"context\": format_docs(relevant_docs), \n",
    "            \"query\": query\n",
    "        }\n",
    "    )\n",
    "    dataset.append(\n",
    "        {\n",
    "            \"user_input\": query,\n",
    "            \"retrieved_contexts\": [doc.page_content for doc in relevant_docs],\n",
    "            \"response\": response,\n",
    "            \"reference\": reference,\n",
    "        }\n",
    "    )\n",
    "\n",
    "evaluation_dataset = EvaluationDataset.from_list(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aebfb21-c082-41a7-98ce-b8b99d676efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_llm = LangchainLLMWrapper(llm)\n",
    "\n",
    "result = evaluate(\n",
    "    dataset=evaluation_dataset,\n",
    "    metrics=[LLMContextRecall(), Faithfulness(), FactualCorrectness()],\n",
    "    llm=evaluator_llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8749c916-0375-4dfa-955a-5a72c24acca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd877fb5-dda5-468f-97a1-4fc8b48398bc",
   "metadata": {},
   "source": [
    "## 3. Creating an Agent Workflow in LangGraph  \n",
    "a. Tool Definition  \n",
    "A custom get_pokemon_type tool is defined to return Pokémon types by name.\n",
    "\n",
    "@tool\n",
    "\n",
    "def get_pokemon_type(pokemon_name: str) -> str:\n",
    "\n",
    "  ...  \n",
    "b. LangGraph Construction  \n",
    "A simple agent workflow is created:\n",
    "\n",
    "Agent node → Conditional tool call → Tool node → Back to agent or end\n",
    "Standard ReAct-style flow.\n",
    "\n",
    "\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "workflow.add_node(\"agent\", agent)\n",
    "\n",
    "workflow.add_node(\"tools\", ToolNode(tools))  \n",
    "c. Invocation Example  \n",
    "Input: \"What is the Gengar's type?\"\n",
    "\n",
    "The agent correctly calls the tool and responds: \"ghost/poison\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a034d777-09f6-445c-aa89-cfcd7ff377f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pokemon_types_map = {\n",
    "    \"pikachu\": \"electric\",\n",
    "    \"eevee\": \"normal\",\n",
    "    \"bulbasaur\": \"grass/poison\",\n",
    "    \"squirtle\": \"water\",\n",
    "    \"charizard\": \"fire/flying\",\n",
    "    \"jigglypuff\": \"normal/fairy\",\n",
    "    \"meowth\": \"normal\",\n",
    "    \"psyduck\": \"water\",\n",
    "    \"machamp\": \"fighting\",\n",
    "    \"gengar\": \"ghost/poison\",\n",
    "    \"alakazam\": \"psychic\",\n",
    "    \"snorlax\": \"normal\",\n",
    "    \"dragonite\": \"dragon/flying\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921f15f9-43d3-469f-bef9-6e8330089703",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_pokemon_type(pokemon_name: str) -> str:\n",
    "    \"\"\"Fetches the type of the specified Pokémon.\n",
    "\n",
    "    Args:\n",
    "        pokemon_name : The name of the Pokémon (e.g., 'pikachu', 'charizard', 'eevee').\n",
    "\n",
    "    Returns:\n",
    "        str: The type(s) of the Pokémon.\n",
    "\n",
    "    Raises:\n",
    "        KeyError: If the specified Pokémon is not found in the data source.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pokemon_name = pokemon_name.lower().strip()\n",
    "        if pokemon_name not in pokemon_types_map:\n",
    "            raise KeyError(\n",
    "                f\"Pokémon '{pokemon_name}' not found. Available Pokémon: {', '.join(pokemon_types.keys())}\"\n",
    "            )\n",
    "        return pokemon_types_map[pokemon_name]\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error fetching Pokémon type: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3735f62-1e0f-42dd-8255-c520aaca0ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [get_pokemon_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c9d7d6-f621-4c2f-8764-751feabaed60",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8a1f33-db94-4fce-ad23-d966e54a0706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(state: MessagesState):\n",
    "    ai_message = llm_with_tools.invoke(state[\"messages\"])\n",
    "    return {\"messages\": ai_message}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d37bedd-88cf-4199-9234-2b29dd945b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def router(state: MessagesState):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "\n",
    "    return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218306f1-d476-4845-bee9-c90e1bf334c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "workflow.add_node(\"agent\", agent)\n",
    "workflow.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "workflow.add_edge(START, \"agent\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    source=\"agent\", \n",
    "    path=router, \n",
    "    path_map=[\"tools\", END]\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"tools\", \"agent\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9819667e-f235-4c55-a2cb-c3e62eb52a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = workflow.compile()\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        graph.get_graph().draw_mermaid_png()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1204a7-9d09-4c6b-bccd-9510544e3cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = graph.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What is the Gengar's type?\")]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a861cf5e-1d3d-44e0-9675-3958ae60a001",
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in result[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf2b20e-0b36-4b0e-9f9c-99255153e582",
   "metadata": {},
   "source": [
    "## converting to RAGAS\n",
    "\n",
    "## 4. Evaluating the Agent with RAGAS  \n",
    "a. Conversion to RAGAS Format  \n",
    "LangGraph message trace is converted to RAGAS-compatible format using convert_to_ragas_messages().\n",
    "\n",
    "ragas_trace = convert_to_ragas_messages(result[\"messages\"])\n",
    "b. Tool Call Accuracy  \n",
    "Measured using ToolCallAccuracy metric.\n",
    "\n",
    "Reference includes expected tool call and arguments.\n",
    "\n",
    "Result: 100%\n",
    "\n",
    "c. Agent Goal Accuracy  \n",
    "Measures whether the agent achieved the intended result.\n",
    "\n",
    "Uses AgentGoalAccuracyWithReference.\n",
    "\n",
    "Result: 100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db76454d-481e-4f0a-aee6-f9af8064f65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ragas_trace = convert_to_ragas_messages(result[\"messages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b010b30-863e-4be7-bde0-417b1cb65cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ragas_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d134c79d-5e76-4d3e-b676-ddb2106946b9",
   "metadata": {},
   "source": [
    "## Evaluate tool use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d9696a-fb3f-45f6-aa40-1383bbca7256",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = MultiTurnSample(\n",
    "    user_input=ragas_trace,\n",
    "    reference_tool_calls=[\n",
    "        ragas_messages.ToolCall(\n",
    "            name=\"get_pokemon_type\", \n",
    "            args={\"pokemon_name\": \"gengar\"}\n",
    "        )\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d289ad-f045-48c1-a986-9f6825eb43c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = ToolCallAccuracy()\n",
    "scorer.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9a166b-8b6f-44f0-bbf2-1d631202765d",
   "metadata": {},
   "outputs": [],
   "source": [
    "await scorer.multi_turn_ascore(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b593fe-665e-4a4b-9619-0edd638cb17a",
   "metadata": {},
   "source": [
    "## Evaluate Agent Goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47826254-cc95-47c8-a993-82ef007eda74",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = MultiTurnSample(\n",
    "    user_input=ragas_trace,\n",
    "    reference=\"What is the Gengar's type?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579ece39-121a-484b-8fd9-a21103f51439",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = AgentGoalAccuracyWithReference()\n",
    "scorer.llm = LangchainLLMWrapper(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc8580f-07fa-4412-9d67-a7fbc69631b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "await scorer.multi_turn_ascore(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e0bae3-7b39-4ce2-9c79-56484b595a3a",
   "metadata": {},
   "source": [
    "## 5. Key Concepts Highlighted  \n",
    "RAG pipelines can be evaluated objectively for performance and correctness using RAGAS.\n",
    "\n",
    "LangGraph workflows with tool calls can be tested just like standard RAG flows.\n",
    "\n",
    "Metrics such as faithfulness, factual correctness, and tool call accuracy enable deep insights into agent behavior.\n",
    "\n",
    "Agent performance can be reliably benchmarked and improved iteratively.\n",
    "\n",
    "## 6. Conclusion  \n",
    "RAGAS is a powerful toolkit for evaluating the effectiveness of both retrieval-based systems and agent workflows.\n",
    "\n",
    "This demo shows how to apply evaluation metrics in both traditional and tool-augmented pipelines to ensure system quality and correctness."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
