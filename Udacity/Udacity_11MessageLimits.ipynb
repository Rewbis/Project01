{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "108f4737-05cd-465d-a680-fb5d50db8d08",
   "metadata": {},
   "source": [
    "Demo - limiting the number of tokens used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ab73eb-9418-4748-a0b7-feac36ef1aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Annotated\n",
    "from langgraph.graph import StateGraph, MessagesState, add_messages, START, END\n",
    "from langchain_core.messages import (\n",
    "    HumanMessage, \n",
    "    AIMessage, \n",
    "    SystemMessage,\n",
    "    AnyMessage,\n",
    "    RemoveMessage,\n",
    "    trim_messages\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c87d7d-7047-4ebc-941a-0a859e355cd4",
   "metadata": {},
   "source": [
    "**Single Node Workflow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f66dbde-61e3-4339-b0be-8720c90f04a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ddf6ed-6d1c-4c00-be14-414a8bd4783d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_node(state: MessagesState):\n",
    "    return {\"messages\": llm.invoke(state[\"messages\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c144cdf-7f76-4823-9082-0d31d06d3d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(MessagesState)\n",
    "workflow.add_node(\"llm_node\", llm_node)\n",
    "workflow.add_edge(START, \"llm_node\")\n",
    "workflow.add_edge(\"llm_node\", END)\n",
    "graph = workflow.compile()\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        graph.get_graph().draw_mermaid_png()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b96b5c5-f2af-43c0-9c06-7f50636701df",
   "metadata": {},
   "source": [
    "**Useful Messages List**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d249ad-a8fb-4f70-8a8e-05aa583a9f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"You're a FinTech specialist. You're not allowed to talk about anything else. Be concise in your answers.\",\n",
    "        name=\"System\",\n",
    "        id=\"0\",\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"What is Pokemon?\",\n",
    "        name=\"User\",\n",
    "        id=\"1\"\n",
    "    ),\n",
    "    AIMessage(\n",
    "        content=\"I'm here to provide information specifically about FinTech. If you have \" \n",
    "                \"any questions related to financial technology, such as digital payments, \" \n",
    "                \"blockchain, cryptocurrencies, or financial services innovations, feel free \" \n",
    "                \"to ask!\",\n",
    "        name=\"FintechAssistant\",\n",
    "        id=\"2\",\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"What is BlockChain?\",\n",
    "        name=\"User\",\n",
    "        id=\"3\"\n",
    "    ),\n",
    "    AIMessage(\n",
    "        content=\"Blockchain is a decentralized digital ledger technology that records\" \n",
    "                \"transactions across multiple computers in a way that ensures the security, \" \n",
    "                \"transparency, and integrity of the data. Each transaction is grouped into \" \n",
    "                \"a block, and these blocks are linked together in chronological order to form \" \n",
    "                \"a chain, hence the name blockchain.\",\n",
    "        name=\"FintechAssistant\",\n",
    "        id=\"4\",\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"What is a credit card fraud?\",\n",
    "        name=\"User\",\n",
    "        id=\"5\"\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f652f7-4532-4640-be66-c74c4cd3b84c",
   "metadata": {},
   "source": [
    "**Filter Messages during Invocation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fac3ae-e25c-4c44-aabb-8d8b0164297e",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_output = graph.invoke({\"messages\": messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cfb2a0-9c1e-447d-b9ea-811f28b9b8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db5933b-dff4-410c-a787-092cba5f239e",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_output[\"messages\"][-1].response_metadata[\"token_usage\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e622a3-5d4c-47c7-8dd4-a5ca32330658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this one with only the first and final message (initial setup prompt and last human prompt)\n",
    "filtered_output = graph.invoke({\"messages\": [messages[0], messages[-1]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcbf86c-7280-4e95-96d0-778ffff3baa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f465233-24eb-45f6-ba09-e3d5f83ea83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_output[\"messages\"][-1].response_metadata[\"token_usage\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edc01f3-37d7-496f-bca9-9b17eeb2b0cc",
   "metadata": {},
   "source": [
    "Filter Messages inside a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfe6091-4d0a-4d34-9a0f-929c4c7da2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(MessagesState):\n",
    "    filtered_messages: Annotated[List[AnyMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fdd029-9666-413b-a6d0-46d43bd84bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_node(state: State):\n",
    "    filtered_messages = state[\"messages\"][-3:]\n",
    "    ai_message = llm.invoke(filtered_messages)\n",
    "    filtered_messages.append(ai_message)\n",
    "    return {\"messages\": ai_message, \"filtered_messages\": filtered_messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6119ca-4d7e-4a08-8078-dd6fb2b76ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"llm_node\", llm_node)\n",
    "workflow.add_edge(START, \"llm_node\")\n",
    "workflow.add_edge(\"llm_node\", END)\n",
    "graph = workflow.compile()\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        graph.get_graph().draw_mermaid_png()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22031c10-999a-406f-9a78-46cb7d8f63ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in messages:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab741e2d-bc36-4c56-a0de-62e0ea73ea20",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = graph.invoke({'messages': messages})\n",
    "for m in output['filtered_messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22c7910-a11c-4ffe-875b-95d5c5029be2",
   "metadata": {},
   "source": [
    "**Remove Messages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad26ea18-4469-4936-88ac-b8cf1236d31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages[:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc4cb4a-4929-4f6d-8faa-9c9cc4378ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_messages = [RemoveMessage(id=m.id) for m in messages[:-3]]\n",
    "add_messages(messages, delete_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b1c0f8-e423-43e1-8c94-2cd1fc8f51c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(MessagesState):\n",
    "    filtered_messages: Annotated[List[AnyMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f78f153-896f-4176-871d-919258381058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove messages from list (but not system messages)\n",
    "def removal_filter(state: State):\n",
    "    filtered_messages = [\n",
    "        RemoveMessage(id=m.id) \n",
    "            for m in state[\"messages\"][:-3] \n",
    "            if m.name != \"System\"\n",
    "    ]\n",
    "    return {\n",
    "        \"filtered_messages\": add_messages(\n",
    "            state[\"messages\"], \n",
    "            filtered_messages\n",
    "        )\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01764d64-f572-44c0-9745-f3dcc129f6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_node(state: State):\n",
    "    ai_message = llm.invoke(state[\"filtered_messages\"])\n",
    "    return {\n",
    "        \"filtered_messages\": ai_message,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741e5d7d-56ae-4227-93a3-717ae0f37fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"llm_node\", llm_node)\n",
    "workflow.add_node(\"removal_filter\", removal_filter)\n",
    "workflow.add_edge(START, \"removal_filter\")\n",
    "workflow.add_edge(\"removal_filter\", \"llm_node\")\n",
    "workflow.add_edge(\"llm_node\", END)\n",
    "\n",
    "graph = workflow.compile()\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        graph.get_graph().draw_mermaid_png()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd59df8d-b0d3-43fe-b7a2-46ba0cacaf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in messages:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d421d97-4ff3-4d88-845f-18c00ff07625",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = graph.invoke({'messages': messages})\n",
    "for m in output['filtered_messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708af413-379d-4816-86f4-680d783fba4c",
   "metadata": {},
   "source": [
    "Trim Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8950013-578c-4a33-b02b-b1626e51712f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeps last few tokens in the messages list (30 tokens here)\n",
    "trim_messages(\n",
    "    messages,\n",
    "    max_tokens=30,\n",
    "    strategy=\"last\",\n",
    "    token_counter=llm,\n",
    "    allow_partial=False,\n",
    "    include_system=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e254eca9-d91c-4a1a-b649-32dadc55c08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(MessagesState):\n",
    "    max_tokens: int\n",
    "    filtered_messages: Annotated[List[AnyMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2a6329-d970-4da9-a5ff-0382375b1f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_filter(state: State):\n",
    "    max_tokens = state[\"max_tokens\"]\n",
    "    messages = state[\"messages\"]\n",
    "    filtered_messages = messages\n",
    "    if max_tokens:\n",
    "        filtered_messages = trim_messages(\n",
    "            messages=messages,\n",
    "            max_tokens=max_tokens,\n",
    "            strategy=\"last\",\n",
    "            token_counter=llm,\n",
    "            include_system=True,\n",
    "            allow_partial=False\n",
    "        )\n",
    "    return {\"filtered_messages\": filtered_messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcc40f0-b056-46cf-9d59-bd763ecfb52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_node(state: State):\n",
    "    return {\"filtered_messages\": llm.invoke(state[\"filtered_messages\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a257ff-eab9-4fdd-beb1-20ebd5034628",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"llm_node\", llm_node)\n",
    "workflow.add_node(\"trim_filter\", trim_filter)\n",
    "workflow.add_edge(START, \"trim_filter\")\n",
    "workflow.add_edge(\"trim_filter\", \"llm_node\")\n",
    "workflow.add_edge(\"llm_node\", END)\n",
    "\n",
    "graph = workflow.compile()\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        graph.get_graph().draw_mermaid_png()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461c65f2-09ca-4db6-8d34-4a1a5885dce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in messages:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf82b1de-ca40-4bea-a65a-25b6077cba56",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = graph.invoke(\n",
    "    input={\n",
    "        \"max_tokens\": 50,\n",
    "        \"messages\": messages\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75eeed80-b689-4d76-ba25-718274d51895",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in output['filtered_messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5356ee-1f4d-4643-b34a-9ed7ca5d3b50",
   "metadata": {},
   "source": [
    "Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68920562-a725-4709-a46b-ceecb58f9d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5dc0b2-348e-418a-a063-86a54d0c1c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim off the first and last message\n",
    "messages_to_summarize = messages[1:-1]\n",
    "summary_message = HumanMessage(\n",
    "    content=\"Create a summary of the conversation above:\", \n",
    "    name=\"User\"\n",
    ")\n",
    "# get it summarised\n",
    "ai_message = llm.invoke(\n",
    "    add_messages(\n",
    "        messages_to_summarize,\n",
    "        summary_message\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff5c14e-ee41-4065-a5a7-9ec439aa753f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c284ef7-06fc-48f5-aa38-9bf3c467b9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_message.id = \"1\"\n",
    "messages[-1].id = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530e484b-4b71-40e4-9306-f80e07393a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary of middle messages, addingin the outer messages, much like AI realm\n",
    "remaining_messages = [messages[0]] + [ai_message] + [messages[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7bfa28-163e-4718-a863-696632adeec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedee91a-ab1b-43ea-81d0-96e154395f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_messages.append(llm.invoke(remaining_messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3ab22c-c8b1-41c9-8f88-7ab20d33eb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in remaining_messages:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6b1e64-9829-46d5-b48c-583c609faf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_messages[-1].response_metadata[\"token_usage\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1968033-488a-4411-a305-a8201e886b0e",
   "metadata": {},
   "source": [
    "Summary: Techniques for Limiting Messages to Save Tokens  \n",
    "Overview  \n",
    "This demo explores multiple techniques for limiting messages in LangGraph workflows. Reducing message history is important for saving tokens and optimizing performance in agentic applications that use LLMs over multi-turn conversations.\n",
    "  \n",
    "Key Steps Covered  \n",
    "1. Initial Setup  \n",
    "A single-node workflow is created with:\n",
    "A system message (\"You are a FinTech specialist\").\n",
    "Few-shot examples of human and AI messages (e.g., \"What is Pokémon?\" → refusal to answer).\n",
    "messages = [\n",
    "  SystemMessage(...),\n",
    "  HumanMessage(...),\n",
    "  AIMessage(...),\n",
    "  HumanMessage(...),\n",
    "]\n",
    "Each message is assigned a unique ID for easier filtering later.  \n",
    "2. Token Usage Without Trimming  \n",
    "When invoking with the full message history:\n",
    "Prompt tokens and completion tokens are relatively high (e.g., ~239 total tokens).\n",
    "response = llm.invoke(messages)  \n",
    "3. Simple Manual Filtering  \n",
    "Manually invoking the LLM with only a subset of the messages (e.g., first and last message).\n",
    "This substantially reduces token usage (e.g., ~96 total tokens).\n",
    "response = llm.invoke([messages[0], messages[-1]])  \n",
    "4. Filtering Inside the Node  \n",
    "A custom state is created by extending MessageState, including:\n",
    "messages\n",
    "filtered_messages\n",
    "Inside the node, only the last three messages are passed to the LLM.\n",
    "state = {\"messages\": [...], \"filtered_messages\": state[\"messages\"][-3:]}\n",
    "This avoids needing to manually slice messages each time.  \n",
    "5. Using Remove Messages Strategy  \n",
    "The remove_message reducer from LangGraph is used to delete unwanted messages based on IDs.\n",
    "A deletion list is created to filter out irrelevant few-shot examples while preserving essential context.\n",
    "from langgraph.reducers import remove_messages\n",
    "\n",
    "delete_messages = [\"id_of_old_message\", \"id_of_another_old_message\"]\n",
    "messages = remove_messages(existing_messages, delete_messages=delete_messages)\n",
    "This helps refine conversation history efficiently.  \n",
    "6. Using Trim Messages for Token Limits  \n",
    "Trim strategy is introduced to limit messages by token budget:\n",
    "Keeps only the latest messages that fit within a specified token limit.\n",
    "The trim_messages() method is used with different max token thresholds.\n",
    "trimmed = trim_messages(messages, max_tokens=250, strategy=\"last\")\n",
    "Behavior:\n",
    "\n",
    "Higher token limits keep more conversation history.\n",
    "Lower token limits progressively discard older messages.\n",
    "Examples:\n",
    "\n",
    "250 tokens → retains last two messages.\n",
    "30 tokens → retains only system message.  \n",
    "7. Summarization to Compress Messages  \n",
    "Messages between the initial system message and the latest user query are summarized.\n",
    "Summarization is prompted by inserting a special HumanMessage: \"Summarize the above conversation.\"\n",
    "This produces a concise summary that replaces multiple older turns.\n",
    "summarized_message = llm.invoke(summary_prompt)\n",
    "The resulting list:\n",
    "\n",
    "SystemMessage\n",
    "Summary AIMessage\n",
    "Most recent HumanMessage\n",
    "Token usage drops significantly after summarization.\n",
    "  \n",
    "8. Key Concepts Highlighted  \n",
    "Manual slicing reduces input size but needs management.\n",
    "Automatic filtering inside nodes allows persistent behavior.\n",
    "Reducers (remove_messages, add_messages) provide fine-grained control.\n",
    "Trimming by token count ensures fitting into LLM token limits dynamically.\n",
    "Summarization reduces message volume while retaining conversation context.  \n",
    "9. Conclusion  \n",
    "Efficient management of conversation history is essential for scalable LLM applications.\n",
    "LangGraph offers flexible techniques to balance memory retention with token usage constraints.\n",
    "Combining filtering, trimming, and summarization enables smooth long-running agentic workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8802ade-5d2b-4e50-b074-5b28c1b8ea65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7dee0b0-fd78-4299-9de9-d93d552ac8d1",
   "metadata": {},
   "source": [
    "L3_demo_07_multiple_schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630f6b29-14d8-4b52-9819-2d0ed7d35acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42621b23-63ee-4af6-837f-122347b6481f",
   "metadata": {},
   "source": [
    "Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0d855b-25d0-4986-9aaa-a0801b114d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessState(TypedDict):\n",
    "    input: str\n",
    "    output: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055b67bf-87a8-49b2-ada4-9e827ff9db0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenState(TypedDict):\n",
    "    thought: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731cba07-b1dd-4a7d-bb2f-217c417388c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_a(state: ProcessState) -> HiddenState:\n",
    "    input_value = state[\"input\"]\n",
    "    print(f\"NODE A:\\n \"\n",
    "        f\"->input:{input_value}\\n \" \n",
    "    )\n",
    "    return {\"thought\": f\"I don't know what to do with with this message\"}\n",
    "\n",
    "def node_b(state: HiddenState) -> ProcessState:\n",
    "    hidden_thought = state[\"thought\"]\n",
    "    print(f\"NODE B:\\n \"\n",
    "        f\"->hidden_thought:{hidden_thought}\\n \" \n",
    "    )\n",
    "    return {\"output\": \"Thank you for your message! We're processing it and get back to you soon!\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e8336f-8f9c-400d-8739-0dbcd9ed75d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(ProcessState)\n",
    "workflow.add_node(node_a)\n",
    "workflow.add_node(node_b)\n",
    "workflow.add_edge(START, \"node_a\")\n",
    "workflow.add_edge(\"node_a\", \"node_b\")\n",
    "workflow.add_edge(\"node_b\", END)\n",
    "graph = workflow.compile()\n",
    "display(\n",
    "    Image(\n",
    "        graph.get_graph().draw_mermaid_png()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dede23dc-860c-41e3-85f7-c2cfa8e71970",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.invoke({\"input\" : \"The product doesn't work. I want my money back!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e511f9-e306-478d-a672-003f971fcb47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2ab6a31-0ff7-4cd4-9f5d-7b20d91d2b94",
   "metadata": {},
   "source": [
    "**StateGraph with Input and Output**\n",
    "Three schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6efdd67-9ce0-4406-8468-2655f3c9b4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputState(TypedDict):\n",
    "    input: str\n",
    "\n",
    "class OutputState(TypedDict):\n",
    "    output: str\n",
    "\n",
    "class ProcessState(TypedDict):\n",
    "    input: str\n",
    "    thought: str\n",
    "    output: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908a5642-060e-4d9c-80b2-15fdac9752de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_agent(state: InputState):\n",
    "    input_value = state[\"input\"]\n",
    "    print(f\"NODE A:\\n \"\n",
    "        f\"->input:{input_value}\\n \" \n",
    "    )\n",
    "    return {\n",
    "        \"output\": \"Thank you for your message!\",\n",
    "        \"thought\": \"An L2 Agent should take care of this\"\n",
    "    }\n",
    "\n",
    "def l2_agent(state: ProcessState) -> OutputState:\n",
    "    l1_output = state[\"output\"]\n",
    "    hidden_thought = state[\"thought\"]\n",
    "    print(f\"NODE B:\\n \"\n",
    "        f\"->l1_output:{l1_output}\\n \"\n",
    "        f\"->hidden_thought:{hidden_thought}\\n \" \n",
    "    )\n",
    "    return {\n",
    "        \"output\": f\"{l1_output} We're processing it and get back to you soon!\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20a40c6-e840-4f51-872f-f4b9c72a6a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# workflow = StateGraph(ProcessState)\n",
    "workflow = StateGraph(ProcessState, input=InputState, output=OutputState)\n",
    "workflow.add_node(l1_agent)\n",
    "workflow.add_node(l2_agent)\n",
    "workflow.add_edge(START, \"l1_agent\")\n",
    "workflow.add_edge(\"l1_agent\", \"l2_agent\")\n",
    "workflow.add_edge(\"l2_agent\", END)\n",
    "graph = workflow.compile()\n",
    "display(\n",
    "    Image(\n",
    "        graph.get_graph().draw_mermaid_png()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b37ef73-14fc-471e-abb7-baee97dfadf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.invoke({\"input\" : \"The product doesn't work. I want my money back!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfffbe0-0865-4144-a6fb-d5582dd5f3d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f54439-013a-4fc0-b915-45b96a91c6ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1cf98f-0833-478c-9b95-f394651a2d5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf676d0-0956-499b-ab62-b862f195b182",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8487c52-4755-46b1-8cd7-00ccfc18907b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8711d459-9663-44bc-962e-bd0320a51914",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
