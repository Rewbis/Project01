{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7292c138-d2b1-4ff8-8820-d00c2aa2772c",
   "metadata": {},
   "source": [
    "First, functions and tools:\n",
    "Summary: Using Tools with LangChain\n",
    "Overview\n",
    "This demo shows how to create and integrate tools into LangChain workflows. Tools allow the LLM to delegate specific tasks to external functions by invoking them during the conversation, enabling a powerful method for function calling within AI applications.\n",
    "\n",
    "Key Steps Covered\n",
    "1. Tool Creation\n",
    "Tools are simple Python functions decorated with @tool.\n",
    "A good description must be provided so the LLM can understand when to call it.\n",
    "@tool(\"multiply\")\n",
    "def multiply(x: int, y: int) -> int:\n",
    "  \"\"\"Multiply two numbers together.\"\"\"\n",
    "  return x * y\n",
    "The tool is registered into a tools list and a tool map for easy lookup by name.\n",
    "tools = [multiply]\n",
    "tool_map = {tool.name: tool for tool in tools}\n",
    "2. Binding Tools to an LLM\n",
    "Tools are bound to the LLM using bind_tools().\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "This makes the LLM aware that it can call tools during a conversation if needed.\n",
    "3. Invoking with Tools\n",
    "A typical message list includes:\n",
    "A SystemMessage setting the assistant's behavior.\n",
    "A HumanMessage containing the user query (e.g., \"What is 3 multiplied by 2?\").\n",
    "messages = [\n",
    "  SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "  HumanMessage(content=\"What is 3 multiplied by 2?\")\n",
    "]\n",
    "Invoking the LLM:\n",
    "If the LLM identifies a tool to call, it produces tool_calls instead of a direct text response.\n",
    "Otherwise, it produces regular content.\n",
    "response = llm_with_tools.invoke(messages)\n",
    "Example:\n",
    "\"How are you?\" → Direct response, no tool_calls.\n",
    "\"What is 3 multiplied by 2?\" → Empty content + tool_calls for the multiply function.\n",
    "4. Handling Tool Calls Programmatically\n",
    "Tool calls must be handled by the developer:\n",
    "Parse tool_calls from the LLM response using LangChain utilities.\n",
    "Extract the function name, arguments, and call ID.\n",
    "Execute the corresponding tool using the arguments.\n",
    "parsed_calls = parse_tool_calls(response.additional_kwargs[\"tool_calls\"])\n",
    "tool_call = parsed_calls[0]\n",
    "function_name = tool_call[\"name\"]\n",
    "args = tool_call[\"args\"]\n",
    "\n",
    "tool = tool_map[function_name]\n",
    "result = tool.invoke(**args)\n",
    "After executing the tool:\n",
    "A ToolMessage is created with the result, linking it back to the original tool_call ID.\n",
    "tool_message = ToolMessage(content=str(result), tool_call_id=tool_call[\"id\"])\n",
    "messages.append(tool_message)\n",
    "5. Sending the Result Back to the LLM\n",
    "After the tool result is appended to the messages list:\n",
    "A second invocation sends the updated conversation to the LLM.\n",
    "The LLM then generates a final, natural language response incorporating the tool result.\n",
    "final_response = llm_with_tools.invoke(messages)\n",
    "Example final output: \"3 multiplied by 2 is 6.\"\n",
    "6. Summary of the Flow\n",
    "Human asks a question.\n",
    "LLM identifies if a tool is needed.\n",
    "Tool is called manually by the application.\n",
    "Tool output is fed back to the LLM.\n",
    "LLM produces a complete answer.\n",
    "7. Conclusion\n",
    "Tools in LangChain enable structured, reliable, and expandable interaction patterns.\n",
    "Developers must manage tool execution and message flow carefully.\n",
    "This method paves the way for building powerful agentic systems that combine LLM reasoning with external capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c782f559-7dd3-4cd7-8f1b-6ee7d813ad34",
   "metadata": {},
   "source": [
    "Demo: Simple Agentic Workflows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3bd6f7-bc0c-4096-a3c5-a268ba6e6c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import (\n",
    "    AIMessage, \n",
    "    HumanMessage, \n",
    "    SystemMessage, \n",
    "    ToolMessage\n",
    ")\n",
    "from langchain_core.tools.structured import StructuredTool\n",
    "from langchain.tools import tool\n",
    "from langchain_core.output_parsers.openai_tools import parse_tool_calls\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0555357-2d73-47a3-8412-61a5be5cd61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "# if dotenv is set up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60ead6a-286c-467f-9031-55e030283ffd",
   "metadata": {},
   "source": [
    "**The Agent Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c2e9dc-de9f-4f4f-a374-0d319a3e64ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(\n",
    "            self, \n",
    "            name:str=\"AI Agent\",\n",
    "            role:str=\"Personal Assistant\",\n",
    "            instructions:str = \"Help users with any question\", \n",
    "            model:str=\"gpt-4o-mini\",\n",
    "            temperature:float=0.0,            \n",
    "            tools:List[StructuredTool]=[]):\n",
    "        \n",
    "        self.name = name\n",
    "        self.role = role\n",
    "        self.instructions = instructions\n",
    "\n",
    "        self.llm = ChatOpenAI(\n",
    "            model=model,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "\n",
    "        self.tools = tools\n",
    "        self.tool_map = {tool.name:tool for tool in tools}\n",
    "        self.memory = [\n",
    "            SystemMessage(\n",
    "                content=f\"You're {self.name}, your role is {self.role}, \" \n",
    "                        f\"and you need to {self.instructions} \"\n",
    "            ),\n",
    "        ]\n",
    "        \n",
    "    def invoke(self, user_message:str):\n",
    "        self.memory.append(HumanMessage(content=user_message))\n",
    "        ai_message = self._invoke_llm()\n",
    "\n",
    "        tool_calls = ai_message.additional_kwargs.get('tool_calls')\n",
    "        if tool_calls:\n",
    "            self._call_tools(tool_calls)\n",
    "            self._invoke_llm()\n",
    "\n",
    "        return self.memory[-1].content\n",
    "\n",
    "    def _invoke_llm(self)->AIMessage:\n",
    "        llm = self.llm.bind_tools(self.tools)\n",
    "        ai_message = llm.invoke(self.memory)\n",
    "        self.memory.append(ai_message)\n",
    "        return ai_message\n",
    "\n",
    "    def _call_tools(self, tool_calls:List[Dict]):\n",
    "        parsed_tool_calls = parse_tool_calls(tool_calls)\n",
    "        for tool_call in parsed_tool_calls:\n",
    "            tool_call_id = tool_call['id']\n",
    "            function_name = tool_call['name']\n",
    "            arguments = tool_call['args']\n",
    "            func = self.tool_map[function_name]\n",
    "            result = func.invoke(arguments)\n",
    "            tool_message = ToolMessage(\n",
    "                content=result,\n",
    "                name=function_name,\n",
    "                tool_call_id=tool_call_id,\n",
    "            )\n",
    "            self.memory.append(tool_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6786276-751a-4367-9438-880b282f8565",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b5157c-16c6-4c7d-aba4-c9dbfe429633",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    tools=[multiply]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4783cb27-6bf8-40bf-bee9-53eee8b6b6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.invoke(\"2 multiplied by 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d251fd11-73d3-41a0-afbb-eb227e19b9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d1c37e-7ce0-441e-85b1-38ef8e413449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba603457-84e2-4046-805e-4ccfae2351b7",
   "metadata": {},
   "source": [
    "Summary: Building a Basic Agent Abstraction in LangChain\n",
    "Overview\n",
    "This demo shows how to create a lightweight agent abstraction that layers together LLMs, memory, and tool use. The goal is to allow seamless handling of user inputs, tool calls, and model responses without having to manually orchestrate every piece at each step.\n",
    "\n",
    "Key Steps Covered\n",
    "1. Initial Setup\n",
    "Standard imports and environment loading.\n",
    "OpenAI chat model is instantiated as the LLM.\n",
    "Tools are defined and organized:\n",
    "A simple multiply tool is created.\n",
    "A tool map is built for easy lookup by tool name.\n",
    "@tool(\"multiply\")\n",
    "def multiply(x: int, y: int) -> int:\n",
    "  \"\"\"Multiply two numbers together.\"\"\"\n",
    "  return x * y\n",
    "\n",
    "tools = [multiply]\n",
    "tool_map = {tool.name: tool for tool in tools}\n",
    "2. Memory Initialization\n",
    "A Memory object is created to track the conversation history.\n",
    "The memory is seeded with a SystemMessage to instruct the model on its role and behavior.\n",
    "memory.add_message(\"system\", \"You are a helpful assistant specialized in math operations.\")\n",
    "3. Agent Class Design\n",
    "An Agent class is implemented with the following key features:\n",
    "\n",
    "Constructor\n",
    "Takes parameters like:\n",
    "name, role, instructions\n",
    "model, temperature\n",
    "tools (optional)\n",
    "Sets up the LLM, tools, tool map, and initializes memory.\n",
    "invoke() Method\n",
    "Accepts a user_message input.\n",
    "Appends the user's message to memory.\n",
    "Invokes the LLM with the current memory.\n",
    "Detects if the LLM produced a tool call:\n",
    "If yes, it delegates the action to the call_tool() method.\n",
    "Otherwise, it records the assistant's reply.\n",
    "Returns the final assistant response from the latest memory entry.\n",
    "def invoke(self, user_message):\n",
    "  memory.add_message(\"user\", user_message)\n",
    "  ai_response = llm.invoke(memory.get_messages())\n",
    "  if ai_response.tool_calls:\n",
    "      self.call_tool(ai_response.tool_calls)\n",
    "  else:\n",
    "      memory.add_message(\"assistant\", ai_response.content)\n",
    "  return memory.last_message().content\n",
    "call_tool() Method\n",
    "Handles the parsing of tool calls.\n",
    "Executes the appropriate tool based on the call and appends the result back into memory as a ToolMessage.\n",
    "def call_tool(self, tool_calls):\n",
    "  tool_call = tool_calls[0]\n",
    "  function_name = tool_call.function.name\n",
    "  args = json.loads(tool_call.function.arguments)\n",
    "  tool = self.tool_map[function_name]\n",
    "  result = tool.invoke(**args)\n",
    "  memory.add_message(\"tool\", str(result), tool_call_id=tool_call.id)\n",
    "4. Execution Example\n",
    "An agent is created using default parameters and the multiply tool.\n",
    "\n",
    "User input: \"What is 2 multiplied by 2?\"\n",
    "\n",
    "The agent flow:\n",
    "\n",
    "User input is added to memory.\n",
    "LLM recognizes the need to call the multiply tool.\n",
    "Tool is called and the result (4) is captured.\n",
    "Result is sent back to the LLM.\n",
    "LLM responds naturally: \"2 multiplied by 2 is 4.\"\n",
    "Memory inspection shows:\n",
    "\n",
    "System message\n",
    "Human message\n",
    "AI tool call\n",
    "Tool message with result\n",
    "Final AI message\n",
    "5. Conclusion\n",
    "This basic agent design creates a clean abstraction for:\n",
    "Managing conversation history\n",
    "Handling tool execution\n",
    "Producing coherent LLM responses\n",
    "It's a strong starting point for building more sophisticated, autonomous agent systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d201f6c2-eb7d-4cc5-a81d-cde0c2d26b53",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
