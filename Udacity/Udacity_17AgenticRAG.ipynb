{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14c9c521-95d0-4fe7-953a-d068a699d6d9",
   "metadata": {},
   "source": [
    "## Summary: Introducing Agentic Decisions into RAG Pipelines\n",
    "Overview  \n",
    "This demo expands a basic RAG (Retrieval-Augmented Generation) pipeline to include agentic decision-making. The system dynamically decides whether to rely on retrieved documents or perform live web research depending on the quality of retrieved information.\n",
    "\n",
    "Key Steps Covered  \n",
    "## 1. Setting up the Vector Store  \n",
    "Documents (from a previously used PDF) are loaded into a Chroma vector database.\n",
    "\n",
    "Embeddings are generated and stored.\n",
    "\n",
    "This is the standard offline preprocessing step for RAG.\n",
    "\n",
    "\n",
    "vectorstore = Chroma(...)\n",
    "\n",
    "vectorstore.add_documents(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f94c92-9c38-4932-a794-c17f8be18c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from typing import List, Dict\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage \n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "from langgraph.graph.message import MessagesState\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from IPython.display import Image, display\n",
    "from tavily import TavilyClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaec8ce-70ac-4a04-ad87-ddfc0b1b321d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b778a8-6b83-4084-9738-f319ca728e48",
   "metadata": {},
   "source": [
    "**Loading Documents to VectorDB**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a200a17e-5fa4-4fd0-b0bf-40906be8d4cf",
   "metadata": {},
   "source": [
    "## 2. Defining the State Schema  \n",
    "A custom state is defined including:  \n",
    "\n",
    "search_required: a yes/no flag determining whether a web search is necessary.\n",
    "\n",
    "Other standard RAG fields (messages, question, etc.).\n",
    "\n",
    "\n",
    "class State(MessageState):  \n",
    "\n",
    "  search_required: Optional[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc866e40-0266-436c-b7ba-192cf64f7871",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_fn = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5bcddc-f513-4f36-a3a8-d111dcdc1318",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma(\n",
    "    collection_name=\"udacity\",\n",
    "    embedding_function=embeddings_fn\n",
    ")\n",
    "\n",
    "file_path = \"compact-guide-to-large-language-models.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "pages = []\n",
    "async for page in loader.alazy_load():\n",
    "    pages.append(page)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(pages)\n",
    "\n",
    "_ = vector_store.add_documents(documents=all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9be1ac2-5ef4-4ef0-8a62-12d6c294611e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59fef39-4e9b-4cb1-b8af-a36b05962b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(MessagesState):\n",
    "    question: str\n",
    "    documents: List[Document]\n",
    "    search_required: str = \"NO\"\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacb9d24-15e4-4818-ba24-7c97ad0349c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state: State):\n",
    "    question = state[\"question\"]\n",
    "    retrieved_docs = vector_store.similarity_search(question)\n",
    "    return {\"documents\": retrieved_docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a23b60d-2bac-4d60-afe5-d51c2a5df81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator(state: State):\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    \n",
    "    template = ChatPromptTemplate([\n",
    "        (\"system\", \"You are an assistant for evaluating context.\"),\n",
    "        (\"human\", \"Use the following pieces of retrieved context to determine if an \" \n",
    "                  \"additional search is required. If the context is relevant to the \" \n",
    "                  \"question, your response should be 'NO', i.e. no need for search. \"\n",
    "                  \"If the context is not relevant, say 'YES', i.e. search the web for an answer. \" \n",
    "                  \"Don't explain anything, if it's to search the web, say YES, otherwise, NO. \"\n",
    "                  \"\\n# Question: \\n-> {question} \"\n",
    "                  \"\\n# Context: \\n-> {context} \"\n",
    "                  \"\\n# Answer: \"),\n",
    "    ])\n",
    "\n",
    "    chain = template | llm | StrOutputParser()\n",
    "\n",
    "    search_required = chain.invoke(\n",
    "        {\"context\": docs_content, \"question\": question}\n",
    "    )\n",
    "\n",
    "    return {\"search_required\": search_required}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a0142a-874e-4190-8424-69e6ea70c900",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def web_search(question:str)->Dict:\n",
    "    \"\"\"\n",
    "    Return top search results for a given search query\n",
    "    \"\"\"\n",
    "    tavily_client = TavilyClient(api_key=os.getenv(\"TAVILY_API_KEY\"))\n",
    "    response = tavily_client.search(question)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265f40c1-14b6-4783-9937-4fec77da49e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = llm.bind_tools([web_search])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c371a9d-272b-4eef-b7b3-c6d04d46766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def researcher(state: State):\n",
    "    question = state[\"question\"]\n",
    "    messages = state[\"messages\"]\n",
    "    if not messages:\n",
    "        human_message = HumanMessage(\n",
    "            \"Conduct a web research to findout the answer to the following question: \"\n",
    "            f\"```{question}```\"\n",
    "        )\n",
    "        messages = [human_message]\n",
    "    \n",
    "    ai_message = llm_with_tools.invoke(messages)\n",
    "    messages.append(ai_message)\n",
    "    return {\"messages\": messages, \"answer\": ai_message.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b81624-2c55-4fcf-98e4-49f8b3e47ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(state: State):\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    \n",
    "    template = ChatPromptTemplate([\n",
    "        (\"system\", \"You are an assistant for question-answering tasks.\"),\n",
    "        (\"human\", \"Use the following pieces of retrieved context to answer the question. \"\n",
    "                \"If you don't know the answer, just say that you don't know. \" \n",
    "                \"Use three sentences maximum and keep the answer concise. \"\n",
    "                \"\\n# Question: \\n-> {question} \"\n",
    "                \"\\n# Context: \\n-> {context} \"\n",
    "                \"\\n# Answer: \"),\n",
    "    ])\n",
    "\n",
    "    messages = template.invoke(\n",
    "        {\"context\": docs_content, \"question\": question}\n",
    "    ).to_messages()\n",
    "\n",
    "    return {\"messages\": messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207c8659-11ef-41d5-b54e-704751dc1a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state: State):\n",
    "    ai_message = llm.invoke(state[\"messages\"])\n",
    "    return {\"answer\": ai_message.content, \"messages\": ai_message}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60fa271-e88f-4a18-92b9-81162134dc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_router(state: MessagesState):\n",
    "    search_required = state[\"search_required\"]\n",
    "    if search_required.lower() == \"yes\":\n",
    "        return \"researcher\"\n",
    "\n",
    "    return \"augment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a877bfd-034c-4e9b-a550-8f690b575fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tool_router(state: MessagesState):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if last_message.tool_calls:\n",
    "        return \"web_search\"\n",
    "\n",
    "    return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c28b50-6eef-4e16-800c-a96c4d2407be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bec3cb7-786a-4fcb-a4a4-6c9d44579046",
   "metadata": {},
   "source": [
    "## 3. Creating Nodes  \n",
    "a. Retrieve Node  \n",
    "Performs similarity search on the vector store to find relevant documents.\n",
    "\n",
    "retrieved_docs = retriever.invoke(state.question)  \n",
    "b. Evaluator Node  \n",
    "Uses an LLM chain with a prompt asking:\n",
    "\n",
    "\"Based on retrieved documents, is a web search required? Answer 'yes' or 'no'.\"\n",
    "The LLM's response sets the search_required field.\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([...])\n",
    "\n",
    "evaluator = prompt | llm | StrOutputParser()  \n",
    "c. Researcher Node  \n",
    "If web search is needed, the researcher node uses Tavily’s web search tool.\n",
    "\n",
    "Searches the web and updates messages with search results.\n",
    "\n",
    "\n",
    "web_search_tool = tavily_client.search  \n",
    "d. Augment Node  \n",
    "If web search is not required, retrieved documents are passed to the LLM for direct answering.\n",
    "\n",
    "augment_prompt = ChatPromptTemplate.from_messages([...])\n",
    "\n",
    "augment = augment_prompt | llm | StrOutputParser()  \n",
    "e. Generate Node  \n",
    "Final output generation based on the selected source of context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc006d9-67c7-46d1-963a-66564c5240cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(State)\n",
    "\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"augment\", augment)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "workflow.add_node(\"evaluator\", evaluator)\n",
    "workflow.add_node(\"researcher\", researcher)\n",
    "workflow.add_node(\"web_search\", ToolNode([web_search]))\n",
    "\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"evaluator\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    source=\"evaluator\", \n",
    "    path=rag_router, \n",
    "    path_map=[\"augment\", \"researcher\"]\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    source=\"researcher\", \n",
    "    path=tool_router, \n",
    "    path_map=[\"web_search\", END]\n",
    ")\n",
    "workflow.add_edge(\"web_search\", \"researcher\")\n",
    "\n",
    "workflow.add_edge(\"augment\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a4ad00-830c-4283-af43-456c9cf6a2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = workflow.compile()\n",
    "display(\n",
    "    Image(\n",
    "        graph.get_graph().draw_mermaid_png()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f74ec5-4394-436a-83ec-d94dd4b2c350",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = graph.invoke(\n",
    "    input={\"question\": \"What is Pokemon?\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4e1dc7-7f5e-4225-a813-fe92607188c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0283c8-e7a8-4563-b4f4-1671174d7598",
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in output[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c9e32b-d3dd-4f74-acbf-25a892fd80f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output[\"search_required\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248c642d-a6da-4363-8902-307d7647403c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = graph.invoke(\n",
    "    input={\"question\": \"What is Open Source model?\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf58b4b-01d0-45d4-a4c5-e7651da1b808",
   "metadata": {},
   "outputs": [],
   "source": [
    "output[\"search_required\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5e418f-39d4-45cc-b6a7-222b22c4ce84",
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in output[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1333a9-d132-40c6-b310-cbb31c5641f6",
   "metadata": {},
   "source": [
    "## 4. Defining Router Functions  \n",
    "Two routers manage control flow:  \n",
    "\n",
    "First router:\n",
    "\n",
    "If search_required == \"yes\", route to the researcher.\n",
    "\n",
    "If no, route to augment.\n",
    "\n",
    "\n",
    "def should_search_router(state):\n",
    "\n",
    "  return \"researcher\" if state.search_required == \"yes\" else \"augment\"\n",
    "Second router:\n",
    "\n",
    "After tool use, if more tool calls are needed, loop.\n",
    "\n",
    "Otherwise, terminate.\n",
    "\n",
    "\n",
    "def tool_router(state):\n",
    "\n",
    "  ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f38d1d-7b4b-4374-928e-626f74518802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdb0fc3-86db-4d65-b165-624bb9b4c41b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491d064c-936e-4111-8992-f34c4391c6d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e18cd9f-245a-41e7-949f-56dc74295cbd",
   "metadata": {},
   "source": [
    "## 5. Building the Workflow  \n",
    "Nodes: retrieve, evaluator, researcher, augment, web_search_tool\n",
    "\n",
    "Edges:\n",
    "\n",
    "start → retrieve → evaluator\n",
    "\n",
    "Conditional edge based on search decision.\n",
    "\n",
    "If web search is needed, the researcher loops until termination.\n",
    "\n",
    "Otherwise, augment proceeds directly to answer generation.\n",
    "\n",
    "\n",
    "workflow.add_conditional_edges(\"evaluator\", should_search_router)\n",
    "Workflow is visualized and compiled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86d74a4-1d90-4cbd-a613-27b848a2d406",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0289753-3e45-4122-a47c-88e3d470d1bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e452b69f-dbb1-472f-9f15-cf78a69e1fed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df85406d-1988-4072-8688-3e62456dd781",
   "metadata": {},
   "source": [
    "## 6. Execution Examples  \n",
    "a. Question: \"What is Pokémon?\"  \n",
    "Retrieval fails (no relevant documents).\n",
    "\n",
    "Evaluator responds \"yes\" to search_required.\n",
    "\n",
    "Researcher performs web search.\n",
    "\n",
    "Returns correct answer: \"Pokemon, short for Pocket Monsters...\"\n",
    "\n",
    "b. Question: \"What is open source model?\"  \n",
    "Retrieval succeeds.\n",
    "\n",
    "Evaluator responds \"no\" to search_required.\n",
    "\n",
    "Context is used directly from the document without web search.\n",
    "\n",
    "Answer successfully pulled from offline documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b85aa26-fc3b-46b0-b1e7-a43684811e58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebcbb4f-7bb9-4303-9aec-3b01acc45033",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968d1ead-2094-42ed-8667-5fb17f524560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0428b0b-d49d-4a62-bff0-d038481a8f8b",
   "metadata": {},
   "source": [
    "7. Key Concepts Highlighted\n",
    "Dynamic decision-making inside a RAG workflow.\n",
    "\n",
    "Fallback to web search when knowledge gaps are detected.\n",
    "\n",
    "Agentic behavior: LLM assesses context sufficiency and adapts.\n",
    "\n",
    "Efficient resource use: Web search is only triggered when necessary.\n",
    "\n",
    "8. Conclusion\n",
    "Adding agentic decisions to RAG pipelines increases reliability and adaptability.\n",
    "\n",
    "Blending offline retrieval with online search creates more robust AI systems.\n",
    "\n",
    "This modular, decision-driven design is foundational for advanced AI agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e414d7-b71d-47e8-8a62-100e013d8489",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bc0009-df07-41a2-ace0-c6df8072f962",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
