{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8216490e-9968-4cbe-8fca-387fb51d80c0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ed7056f-53ad-45f5-9221-b83e27422208",
   "metadata": {},
   "source": [
    "1. Chat Model Initialization\n",
    "The OpenAI chat model (ChatOpenAI) is instantiated.\n",
    "Environment variables are loaded to securely pass the API key.\n",
    "Temperature settings can be configured to adjust response creativity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "119e686b-a60b-4f89-99b9-9ab0679cdddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import inspect\n",
    "import json\n",
    "from typing import (\n",
    "    List,\n",
    "    Dict,\n",
    "    Literal,\n",
    "    Callable,\n",
    "    Any,\n",
    "    get_type_hints\n",
    ")\n",
    "\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_core.prompts import PromptTemplate, FewShotPromptTemplate\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI()  # reads OPENAI_API_KEY from env\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.0,  # no api_key arg needed if env var is set, Env var name must be exact: OPENAI_API_KEY (uppercase). If you set openai_api_key anywhere, rename it.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccb67ce4-842b-41d2-a9b4-feb3962f33c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 9, 'total_tokens': 18, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CNgxwciKIaLbx7JmFStesR7a3610m', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--b1a357ed-fda9-4ef9-bd0f-02ae42f1475b-0', usage_metadata={'input_tokens': 9, 'output_tokens': 9, 'total_tokens': 18, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test connection\n",
    "llm.invoke(\"Hello there\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc5681e-c163-4f72-a3c7-e2d13ee98fb0",
   "metadata": {},
   "source": [
    "2. Message Structuring\n",
    "LangChain supports structured message inputs including:\n",
    "SystemMessage: Sets the assistant’s behavior.\n",
    "HumanMessage: Represents user input.\n",
    "AIMessage: Includes prior assistant responses.\n",
    "Including previous assistant responses helps with few-shot learning, as it teaches the LLM how to respond in context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8352eb5e-6d65-4de3-a512-0d68c9bbf086",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [ \n",
    "    SystemMessage(\"You are a geography tutor\"),\n",
    "    HumanMessage(\"What's the capital of Brazil?\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bd7133-aee7-4bd3-8a8a-a38731ddc00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ffa5f1-bcaa-4a59-bacf-e07a2aa1f129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# give it more shot\n",
    "messages = [ \n",
    "    SystemMessage(\"You are a geography tutor\"),\n",
    "    HumanMessage(\"What's the capital of Brazil?\"),\n",
    "    AIMessage(\"The capital of Brazil is Brasília\"),\n",
    "    HumanMessage(\"What's the capital of Canada?\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f45bd0c-6535-4dcf-836f-42d3004f6c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b5ae4d-83bc-4060-af07-36c9eef4ef72",
   "metadata": {},
   "source": [
    "3. Prompt Templates\n",
    "LangChain offers two styles of templated prompting:\n",
    "\n",
    "a. Basic PromptTemplate\n",
    "Defines a single-variable input prompt with a placeholder.\n",
    "  prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Tell me a joke about {topic}\"\n",
    "  )\n",
    "The prompt can be formatted using .format() or .invoke():\n",
    "  formatted_prompt = prompt.format(topic=\"Python\")\n",
    "  llm.invoke(prompt.invoke({\"topic\": \"Python\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212a72b6-6330-4afb-b1dd-7ab4dedc837a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pure python prompt: 01\n",
    "topic = \"Python\"\n",
    "prompt = f\"Tell me a joke about {topic}\"\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5f762e-1b68-49ac-ab13-fdd8b7b2b848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or this \n",
    "prompt = \"Tell me a joke about {topic}\"\n",
    "llm.invoke(prompt.format(topic = \"Python\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dfd649-5b51-424c-bb2f-f9b7400ac658",
   "metadata": {},
   "source": [
    "Prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ec2fa93-0afb-4fbb-8bc5-72d4b68ef5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    template=\"Tell me a joke about {topic}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9be2ef2c-615d-466c-bf12-50632c19eedf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why do Python programmers prefer dark mode?\\n\\nBecause light attracts bugs!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 13, 'total_tokens': 26, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CNh51ZVaeDShv46aWRFnBOrFj7bpO', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--2a0a58d5-9bc3-43f3-a6a1-4023e460dc7f-0', usage_metadata={'input_tokens': 13, 'output_tokens': 13, 'total_tokens': 26, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# put it into an invoke;\n",
    "llm.invoke(\n",
    "    prompt_template.invoke({\"topic\":\"Python\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e87405-4ee7-4965-8255-c4ad68b1aab3",
   "metadata": {},
   "source": [
    "b. Few-Shot PromptTemplate\n",
    "Combines multiple structured examples to guide the LLM’s reasoning.\n",
    "Components:\n",
    "examples: List of example dictionaries (input → thought → output).\n",
    "example_prompt: A PromptTemplate defining the format for each example.\n",
    "suffix: The actual question for the current prompt.\n",
    "\n",
    "examples = [\n",
    "  {\"input\": \"A train leaves City A...\", \"thought\": \"...\", \"output\": \"2 hours\"},\n",
    "  {\"input\": \"A store applies a 20% discount...\", \"thought\": \"...\", \"output\": \"...\"},\n",
    "  ...\n",
    "]\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "  input_variables=[\"input\", \"thought\", \"output\"],\n",
    "  template=\"Question: {input}\\nThought: {thought}\\nResponse: {output}\"\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "  examples=examples,\n",
    "  example_prompt=example_prompt,\n",
    "  suffix=\"Question: {input}\",\n",
    "  input_variables=[\"input\"]\n",
    ")\n",
    "\n",
    "llm.invoke(few_shot_prompt.invoke({\"input\": \"If today is Wednesday, what day will it be in 10 days?\"}))\n",
    "The result shows the model following the reasoning steps provided in the examples and outputting: \"Saturday.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "246fd9c3-98b6-437d-a382-b591e73b2742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# more shot!\n",
    "example_prompt = PromptTemplate(\n",
    "    template=\"Question: {input}\\nThought: {thought}\\nResponse: {output}\"\n",
    ")\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"A train leaves city A for city B at 60 km/h, and another train leaves city B for city A at 40 km/h. If the distance between them is 200 km, how long until they meet?\", \n",
    "        \"thought\": \"The trains are moving towards each other, so their relative speed is 60 + 40 = 100 km/h. The time to meet is distance divided by relative speed: 200 / 100 = 2 hours.\",\n",
    "        \"output\": \"2 hours\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"If a store applies a 20% discount to a $50 item, what is the final price?\", \n",
    "        \"thought\": \"A 20% discount means multiplying by 0.8. So, $50 × 0.8 = $40.\",\n",
    "        \"output\": \"$40\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"A farmer has chickens and cows. If there are 10 heads and 32 legs, how many of each animal are there?\", \n",
    "        \"thought\": \"Let x be chickens and y be cows. We have two equations: x + y = 10 (heads) and 2x + 4y = 32 (legs). Solving: x + y = 10 → x = 10 - y. Substituting: 2(10 - y) + 4y = 32 → 20 - 2y + 4y = 32 → 2y = 12 → y = 6, so x = 4.\",\n",
    "        \"output\": \"4 chickens, 6 cows\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"If a car travels 90 km in 1.5 hours, what is its average speed?\", \n",
    "        \"thought\": \"Speed is distance divided by time: 90 km / 1.5 hours = 60 km/h.\",\n",
    "        \"output\": \"60 km/h\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"John is twice as old as Alice. In 5 years, their combined age will be 35. How old is Alice now?\", \n",
    "        \"thought\": \"Let Alice's age be x. Then John’s age is 2x. In 5 years, their ages will be x+5 and 2x+5. Their sum is 35: x+5 + 2x+5 = 35 → 3x + 10 = 35 → 3x = 25 → x = 8.33.\",\n",
    "        \"output\": \"8.33 years old\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dd748f8-7802-4a52-93e6-fabade375fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Question: {input}\",\n",
    "    input_variables=[\"input\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7eeb36e-18d7-47e0-8739-a9fac4de253f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought: To find the day of the week in 10 days, we can calculate the remainder of 10 divided by 7 (the number of days in a week). 10 ÷ 7 = 1 remainder 3. This means 10 days from Wednesday is 3 days later. Counting from Wednesday: Thursday (1), Friday (2), Saturday (3). \n",
      "\n",
      "Response: Saturday\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\n",
    "    prompt_template.invoke({\"input\":\"If today is Wednesday, what day will it be in 10 days?\"})\n",
    ")\n",
    "print(response.content) # Response should be Saturday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b27cc7-7154-4404-b0dd-d28eace293a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba1043c4-1c8b-40d3-9936-78f1d12bae44",
   "metadata": {},
   "source": [
    "4. Takeaways\n",
    "Prompt engineering in LangChain is modular and powerful.\n",
    "Structured messages and few-shot examples significantly improve LLM response quality.\n",
    "PromptTemplates enable consistent formatting and reusability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983f3780-10a5-47da-85a5-15ccf46db926",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "114b0962-af7f-4328-be0d-460e43bbcc6c",
   "metadata": {},
   "source": [
    "Summary: Chatbot Application with LangChain\n",
    "Overview\n",
    "This exercise walks through the creation of a simple chatbot using LangChain. The focus is on structuring the chatbot class to support memory, managing message formatting via LangChain message objects, and using prompt templates to enable contextual, character-driven interactions.\n",
    "\n",
    "Key Steps Covered\n",
    "1. Setup and Imports\n",
    "Required libraries are imported, including LangChain modules.\n",
    "Environment variables are loaded to securely manage the OpenAI API key.\n",
    "2. Chatbot Class Construction\n",
    "A chatbot class is created with an internal memory (self.messages), which stores a list of messages exchanged during the conversation.\n",
    "The OpenAI chat model is instantiated using LangChain's ChatOpenAI, without explicitly passing the API key thanks to environment configuration.\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "\n",
    "self.llm = ChatOpenAI()\n",
    "self.messages = []\n",
    "3. Implementing the invoke Method\n",
    "The method accepts user input, wraps it in a HumanMessage, appends it to the message list, and invokes the model with the full conversation history.\n",
    "The model's response is wrapped in an AIMessage and also appended to memory.\n",
    "The AI response is returned at the end.\n",
    "def invoke(self, user_input):\n",
    "  human_msg = HumanMessage(content=user_input)\n",
    "  self.messages.append(human_msg)\n",
    "\n",
    "  ai_msg = self.llm(self.messages)\n",
    "  self.messages.append(ai_msg)\n",
    "\n",
    "  return ai_msg.content\n",
    "4. Prompt Template and Message Formatting\n",
    "The prompt uses LangChain’s PromptTemplate with a structure that includes system instructions, human input, and AI output. This template manages consistent formatting for few-shot examples.\n",
    "\n",
    "Sample message flow includes:\n",
    "\n",
    "System message defining personality and role.\n",
    "Human messages as input.\n",
    "AI messages as example responses.\n",
    "5. Creating and Interacting with the Chatbot\n",
    "A bot named BEEP-42 is created, initialized with humorous and thematic system instructions.\n",
    "\n",
    "A series of example interactions are seeded, such as:\n",
    "\n",
    "\"Hello, what is 2+2?\"\n",
    "\"Can you dream?\"\n",
    "\"Why did the robot go to therapy?\"\n",
    "Memory is inspected to confirm the full conversation history is stored correctly with role-based message types.\n",
    "\n",
    "6. Sample Invocation\n",
    "The bot is queried with a playful sequence of questions:\n",
    "\"HAL, is that you?\" → Responds it's not HAL.\n",
    "\"RedQueen from The Terminator?\" → Clarifies different protocols.\n",
    "\"Wall-E?\" → Confirms it's not Wall-E.\n",
    "\"What's the answer for every question?\" → Replies: \"Answer 42.\"\n",
    "7. Next Steps\n",
    "Learners are encouraged to experiment further by:\n",
    "Adjusting the temperature to influence creativity.\n",
    "Modifying the bot’s personality.\n",
    "Expanding the set of few-shot examples for better grounding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a413a5-8829-466b-8429-2de6f7f9bf77",
   "metadata": {},
   "source": [
    "Streaming in Generative AI Applications\n",
    "Streaming enables faster and smoother user experiences in entertainment platforms like Spotify and Netflix, where content plays immediately while additional data is loaded in the background. The same concept applies to Generative AI applications, ensuring low-latency, real-time interactions.\n",
    "\n",
    "Why Streaming Matters in AI Applications\n",
    "• Without streaming, users must wait for the full response to generate, causing delays.\n",
    "\n",
    "• With streaming, output is displayed progressively, reducing perceived latency and improving responsiveness.\n",
    "\n",
    "• Example: ChatGPT streams text word by word, making interactions feel fluid and natural.\n",
    "\n",
    "Streaming in LangChain\n",
    "LangChain provides built-in streaming support through the Runnable Interface, allowing developers to process responses as they are generated.\n",
    "\n",
    "• stream() – Synchronous streaming, suitable for real-time processing.\n",
    "\n",
    "• astream() – Asynchronous streaming, designed for non-blocking workflows.\n",
    "\n",
    "Using stream() for Real-Time Processing -\n",
    "        for chunk in component.stream(some_input):\n",
    "                print(chunk)  # Processes each chunk as it's produced\n",
    "• Enhances chat applications by displaying responses progressively.\n",
    "\n",
    "• Allows interruption if the user no longer needs the full response.\n",
    "\n",
    "• Requires efficient processing to avoid delays between chunks.\n",
    "\n",
    "Using astream() for Asynchronous Streaming\n",
    "Works similarly but is optimized for async applications, ensuring smooth, non-blocking execution.\n",
    "\n",
    "Final Thoughts\n",
    "Streaming significantly improves user experience by making LLM applications more responsive. Whether building chatbots, virtual assistants, or interactive AI tools, streaming ensures seamless real-time interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e8ff4e-92ed-4232-8b49-781968aa9a3f",
   "metadata": {},
   "source": [
    "Summary: Streaming Responses in LangChain\n",
    "Overview\n",
    "This demo introduces streaming capabilities in LangChain. Instead of waiting for the full response from the model, the output is streamed token-by-token or chunk-by-chunk. This approach enables real-time feedback, partial result handling, and dynamic response processing.\n",
    "\n",
    "Key Steps Covered\n",
    "1. Initial Setup\n",
    "Necessary libraries are imported and environment variables loaded.\n",
    "The OpenAI chat model (ChatOpenAI) is instantiated as the LLM.\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "Standard use with .invoke() is demonstrated:\n",
    "Sends the full prompt and waits for the complete response.\n",
    "3. Streaming Basics\n",
    "Streaming is enabled by using .stream() instead of .invoke().\n",
    "Responses are received incrementally as chunks.\n",
    "\n",
    "chunks = []\n",
    "for chunk in llm.stream(\"What does FIFA stand for?\"):\n",
    "  chunks.append(chunk)\n",
    "  print(chunk.content, end=\"\")\n",
    "\n",
    "Output begins immediately instead of waiting for full completion.\n",
    "Each chunk is an AIMessageChunk with content and additional metadata.\n",
    "3. Working with Chunks\n",
    "Chunks can be processed individually:\n",
    "Slicing chunks (e.g., first 5 chunks) to form partial outputs.\n",
    "Concatenating chunks to recreate the full final output.\n",
    "\n",
    "complete_output = \"\".join(chunk.content for chunk in chunks)\n",
    "\n",
    "4. Handling Interruptions\n",
    "It is possible to interrupt a streaming response.\n",
    "A KeyboardInterrupt is caught to gracefully stop streaming.\n",
    "If interruption handling is disabled, the raw exception is displayed.\n",
    "\n",
    "try:\n",
    "  for chunk in llm.stream(\"Question...\"):\n",
    "      print(chunk.content, end=\"\")\n",
    "except KeyboardInterrupt:\n",
    "  print(\"Interrupted!\")\n",
    "\n",
    "5. Resuming After Interruptions\n",
    "A simple play() and resume() mechanism is demonstrated:\n",
    "play() appends streamed chunks to memory.\n",
    "resume() prompts the model to complete a previously interrupted response.\n",
    "\n",
    "def play():\n",
    "  # Streams response and stores in memory\n",
    "\n",
    "def resume():\n",
    "  # Resumes based on memory if output seems incomplete\n",
    "\n",
    "If the model believes the prior output is unfinished, it continues the answer.\n",
    "6. On-the-Fly Processing\n",
    "Words are counted dynamically during streaming.\n",
    "Each new word token can trigger updates or calculations.\n",
    "\n",
    "word_count = 0\n",
    "for chunk in llm.stream(\"Prompt...\"):\n",
    "  word_count += len(chunk.content.split())\n",
    "\n",
    "This shows how streaming enables real-time processing and metric calculation.\n",
    "7. Event Handling\n",
    "Events are emitted during streaming:\n",
    "on_chat_model_start\n",
    "on_chat_model_stream\n",
    "on_chat_model_end\n",
    "Listeners can be attached to these events to trigger additional actions.\n",
    "Example:\n",
    "After on_chat_model_end, a different process could be initiated.\n",
    "8. Using Streaming in a Chatbot\n",
    "The BEEP-42 chatbot is re-created with streaming enabled.\n",
    "It now outputs text progressively during a conversation, creating a more interactive user experience.\n",
    "\n",
    "bot = Chatbot(name=\"BEEP-42\", instructions=\"...\", examples=[...])\n",
    "response = bot.ask(\"Tell me a joke.\")\n",
    "\n",
    "9. Conclusion\n",
    "Streaming offers faster, more interactive, and more flexible user experiences.\n",
    "Processing data as it arrives enables more sophisticated applications like real-time dashboards, live feedback systems, or conversational agents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1d99a2-efa0-4bfe-9099-45c1c25d0ad0",
   "metadata": {},
   "source": [
    "Summary: Structured Output Parsing with LangChain\n",
    "Overview\n",
    "This demo focuses on how to parse and structure outputs from LLMs using LangChain's output parsers, including strategies for handling structured text like dictionaries, booleans, datetimes, and using Pydantic models for robustness. It also covers how to fix parsing errors automatically when the LLM output is misformatted.\n",
    "\n",
    "Key Steps Covered\n",
    "1. Basic String Parsing\n",
    "By default, calling .invoke() on an LLM returns an AIMessage object.\n",
    "To extract the raw text, access ai_message.content.\n",
    "\n",
    "response = llm.invoke(\"Hello there.\")\n",
    "raw_text = response.content\n",
    "\n",
    "Alternatively, a StrOutputParser can be used to transform the output cleanly.\n",
    "\n",
    "parser = StrOutputParser()\n",
    "text = parser.invoke(response)\n",
    "\n",
    "3. Datetime Parsing\n",
    "A DatetimeOutputParser is used when you need to convert LLM output into a Python datetime object.\n",
    "The LLM is prompted to produce a date in a specific format.\n",
    "\n",
    "parser = DatetimeOutputParser()\n",
    "datetime_obj = parser.invoke(response)\n",
    "\n",
    "4. Boolean Parsing\n",
    "A BooleanOutputParser converts \"yes\" or \"no\" responses into Python True or False.\n",
    "Example:\n",
    "Content: \"yes\" → True\n",
    "Content: \"no\" → False\n",
    "\n",
    "parser = BooleanOutputParser()\n",
    "result = parser.invoke(AIMessage(content=\"yes\"))\n",
    "\n",
    "5. TypedDict Parsing\n",
    "LangChain supports using TypedDict to define the structure of expected output.\n",
    "\n",
    "class UserInfo(TypedDict):\n",
    "  name: str\n",
    "  country: str\n",
    "\n",
    "Using with_structured_output(UserInfo), the model is guided to format its response accordingly.\n",
    "Examples:\n",
    "\n",
    "Input: \"My name is Henrique and I am from Brazil.\" → { \"name\": \"Henrique\", \"country\": \"Brazil\" }\n",
    "\n",
    "If no relevant info is found, defaults are used.\n",
    "6. Pydantic Parsing\n",
    "For more robust parsing and validation, Pydantic models are used.\n",
    "\n",
    "class UserInfo(BaseModel):\n",
    "  name: str\n",
    "  country: str\n",
    "\n",
    "Pydantic models provide automatic type checking and better error handling.\n",
    "\n",
    "parsed = llm.with_structured_output(UserInfo).invoke(\"My name is Washington and I am from Australia.\")\n",
    "\n",
    "If the LLM output is properly structured, parsing succeeds.\n",
    "If missing information, fields default to empty strings or None, based on model configuration.\n",
    "7. Parsing Complex Structures\n",
    "A more complex example is parsing a list of films (filmography) for an actor using a Pydantic model.\n",
    "\n",
    "class Performer(BaseModel):\n",
    "  name: str\n",
    "  film_names: List[str]\n",
    "\n",
    "Asking for \"Scarlett Johansson filmography\" returns the correct structured object with movie names.\n",
    "8. Handling Parsing Errors\n",
    "Sometimes the LLM outputs poorly formatted JSON or semi-structured text.\n",
    "If parsing fails (e.g., bad quotes, wrong format), an OutputParserException is raised.\n",
    "\n",
    "try:\n",
    "  parser.invoke(bad_output)\n",
    "except OutputParserException as e:\n",
    "  print(\"Parsing error caught!\")\n",
    "\n",
    "9. Fixing Misformatted Outputs Automatically\n",
    "LangChain provides an OutputFixingParser.\n",
    "This parser:\n",
    "Detects format errors.\n",
    "Attempts to reformat the output using the LLM itself.\n",
    "\n",
    "fixing_parser = OutputFixingParser.from_llm(parser, llm)\n",
    "corrected_output = fixing_parser.invoke(misformatted_output)\n",
    "\n",
    "This enables parsing even from imperfect LLM outputs, making workflows much more reliable.\n",
    "10. Conclusion\n",
    "Structured output parsing transforms unstructured LLM responses into reliable Python objects.\n",
    "TypedDicts and Pydantic models improve structure and validation.\n",
    "Parsers combined with automatic fixing allow workflows to handle imperfect LLM behavior gracefully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d23040-531d-4536-b4ab-ab5d4464960f",
   "metadata": {},
   "source": [
    "The Evolution from Chains to Runnables in LangChain\n",
    "LangChain originally introduced Chains, which allowed developers to build sequential workflows by passing outputs from one step as inputs to the next. Over time, these legacy Chain classes have been deprecated in favor of more flexible and powerful approaches:\n",
    "\n",
    "LCEL (LangChain Expression Language) – A declarative way to compose AI workflows.\n",
    "LangGraph – A framework for agentic workflows with complex state management.\n",
    "Runnables: The New Standard\n",
    "The Runnable interface is now the core building block of LangChain. It standardizes how components—such as LLMs, output parsers, retrievers, and agent workflows—are executed and composed.\n",
    "\n",
    "What Can Runnables Do?\n",
    "\n",
    "Invoke – Process a single input into an output.\n",
    "Batch – Handle multiple inputs at once.\n",
    "Stream – Output data in chunks for real-time processing.\n",
    "Inspect – Access input, output, and configuration details.\n",
    "Compose – Chain multiple Runnables together for complex workflows.\n",
    "Example of invoking a Runnable with custom configuration:\n",
    "\n",
    "some_runnable.invoke(\n",
    "        some_input, \n",
    "        config={\n",
    "            'run_name': 'my_run', \n",
    "            'tags': ['tag1', 'tag2'], \n",
    "            'metadata': {'key': 'value'}   \n",
    "        }\n",
    ")\n",
    "LCEL: The Declarative Approach to Chains\n",
    "LCEL (LangChain Expression Language) enables composing Runnables efficiently using a syntax similar to Linux pipes:\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "Instead of manually managing execution, LCEL automatically optimizes the workflow, making it easier to build scalable AI applications.\n",
    "\n",
    "Final Thoughts\n",
    "The shift from legacy Chains to Runnables and LCEL provides greater flexibility, efficiency, and composability. Developers can now build complex AI pipelines with less boilerplate code, focusing on defining workflows rather than managing execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7943656d-70f2-4292-9ea4-5aaff5d66bce",
   "metadata": {},
   "source": [
    "Summary: Chaining Executions and LCEL (LangChain Expression Language)\n",
    "Overview\n",
    "This demo explores chaining multiple LangChain components into structured workflows, culminating in the use of LCEL (LangChain Expression Language) for more concise and flexible chain creation. Learners see how to manually connect prompt templates, LLMs, and parsers into execution sequences and how to inspect and customize these executions.\n",
    "\n",
    "Key Steps Covered\n",
    "1. Foundations: Recap of Key Objects\n",
    "Core components revisited:\n",
    "PromptTemplate: Formats input.\n",
    "ChatOpenAI (LLM): Generates AI messages.\n",
    "StrOutputParser: Converts AI messages into strings.\n",
    "Manual chaining:\n",
    "Prompt is filled with input (e.g., topic: Python).\n",
    "LLM generates a joke about the topic.\n",
    "Parser extracts the response as a string.\n",
    "prompt.invoke({\"topic\": \"Python\"})\n",
    "llm.invoke(prompt_output)\n",
    "parser.invoke(llm_output)\n",
    "2. Understanding Runnables\n",
    "Each component is a runnable, meaning it supports:\n",
    "invoke()\n",
    "batch()\n",
    "stream()\n",
    "Runnables also provide introspection:\n",
    "input_schema, output_schema, and config_schema for validation and structure understanding.\n",
    "print(runnable.input_schema)\n",
    "print(runnable.output_schema)\n",
    "Configuration (config) can be passed during invocation to set metadata like run names and tags.\n",
    "llm.invoke(input, config={\"run_name\": \"demo_run\", \"tags\": [\"demo\", \"lcel\"]})\n",
    "3. Building Chains Manually\n",
    "A RunnableSequence is introduced to combine multiple runnables.\n",
    "Outputs are automatically passed to the next runnable.\n",
    "Example:\n",
    "Prompt → LLM → Parser, wrapped as a single chain.\n",
    "chain = RunnableSequence(first=prompt, middle=llm, last=parser)\n",
    "result = chain.invoke({\"topic\": \"Python\"})\n",
    "Batch execution is supported: multiple topics can be processed at once.\n",
    "results = chain.batch([{\"topic\": \"Python\"}, {\"topic\": \"Football\"}])\n",
    "Diagrams show the step-by-step data flow inside chains.\n",
    "4. Advanced Chain Construction\n",
    "Custom Functions as Runnables:\n",
    "Simple Python functions (like doubling or tripling numbers) are wrapped in runnable form.\n",
    "double = RunnableLambda(lambda x: x * 2)\n",
    "triple = RunnableLambda(lambda x: x * 3)\n",
    "Parallel Execution:\n",
    "RunnableParallel runs multiple runnables simultaneously on the same input.\n",
    "parallel_chain = RunnableParallel(double=double, triple=triple)\n",
    "result = parallel_chain.invoke(3)\n",
    "# Output: {\"double\": 6, \"triple\": 9}\n",
    "5. Introduction to LCEL (LangChain Expression Language)\n",
    "LCEL introduces a pipe (|) syntax to build chains more concisely.\n",
    "Same chain as before, but constructed with just:\n",
    "chain = prompt | llm | parser\n",
    "This is functionally identical to manually creating a RunnableSequence.\n",
    "LCEL enhances readability and composability of chains.\n",
    "6. Summary of Features\n",
    "Single and batched invocation.\n",
    "Streaming support built into runnables.\n",
    "Chain visualization through diagrams.\n",
    "Parallel execution for more complex workflows.\n",
    "LCEL for clean, expressive pipeline construction.\n",
    "7. Conclusion\n",
    "LangChain’s chaining system is flexible and composable.\n",
    "LCEL simplifies construction and visualization of multi-step pipelines.\n",
    "Streaming, parallelism, and structured outputs open the door for building robust AI-driven systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
